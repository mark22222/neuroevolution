{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Sheet\n",
    "Dies ist ein cheat sheet mit geläufigen Dingen für Pytorch und Tensorflow\n",
    "## Pytorch\n",
    "(Ein Einsteiger Tutorial für pytorch ist auch [hier](https://pytorch.org/tutorials/beginner/basics/intro.html) zu finden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Code abschnitt kann genutzt werden, um zu schauen, welche Hardware (für das Training) genutzt werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um inputs an das neuronale Netz geben zu können, müssen diese zunächst als Tensoren gespeichert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "xor_inputs = torch.Tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "# Alternative mit numpy\n",
    "np_array = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "print(xor_inputs)\n",
    "print(x_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein neuronales Netz ist ein Modul, dessen Struktur aus Layer Modulen aufgebaut wird. Durch diese Struktur ist es möglich das neuronale Netz leicht so zu designen, wie man es haben möchte. <br/>\n",
    "Bei ```nn.Flatten``` wird die Eingabe beliebiger Dimension in einen Tensor umgewandelt. Dies kann genutzt werden um Bilder in ein passendes Format zu bringen. <br/>\n",
    "```nn.Sequential``` ist dafür da mehrere Module in der gegebenen Reihenfolge zu speichern. Die Daten werden in dieser Reihenfolge durch die Layer Module gegeben. <br/>\n",
    "Bei ```nn.Linear``` ist der Layer vollständig Verbunden. <br/>\n",
    "Die Aktivierungsfunktionen können als einzelne Layer behandelt werden, benötigen aber keine Dimension als Parameter. Man kann sie aber auch beim implementieren der Forward-Funktion aufrufen. Aktivierungsfunktionen können [hier](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) gefunden werden. <br/>\n",
    "<br/>\n",
    "Es wird außerdem gezeigt, wie man Gewichte initaliseren kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        torch.nn.init.uniform_(self.linear_stack[0].weight, -1.0, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgend wird festgelegt, welche Hardware für das neuronale Netz genutzt werden soll. Außerdem wird gezeigt wie man auf Inhalte des neuronalen Netzes zugreifen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0426, -0.0579,  0.5217,  ..., -0.9621, -0.5584, -0.5798],\n",
      "        [-0.3622,  0.2837,  0.0736,  ..., -0.1286, -0.5135, -0.5226]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0353, 0.0085], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 1.3414e-03, -1.3428e-02, -3.4596e-02,  ..., -2.9251e-02,\n",
      "          4.2676e-02,  8.4966e-03],\n",
      "        [-2.6279e-05, -2.9826e-02,  3.4341e-03,  ...,  4.2719e-02,\n",
      "         -3.9291e-02, -4.0031e-02]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0050, -0.0270], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0376, -0.0406, -0.0386,  ..., -0.0197, -0.0269,  0.0205],\n",
      "        [ 0.0373, -0.0150,  0.0184,  ..., -0.0061, -0.0426, -0.0162]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0244,  0.0303], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt verschiedene Loss-Funktionen, die in der Library vorhanden sind. Außerdem gibt es verschiedene Optimizer, welche die Parameter, die optimiert werden sollen, sowie die learning rate als Parameter bekommen. (Es gibt noch weitere optionale Parameter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Abschnitt zeigt, wie man die Loss-Funktion und den Optimizer einsetzen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(data):\n",
    "    size = len(data)\n",
    "    for batch, (X, y) in enumerate(data):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = torch_loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "(Ein Einsteiger Tutorial für tensorflow ist auch [hier](https://www.tensorflow.org/tutorials/quickstart/beginner) zu finden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kann diese Inputs einfach so an Tensorflow geben, man kann sie aber auch stattdessen in einen Tensor konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]], shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# input X vector\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "# output Y vector\n",
    "Y = [[0], [1], [1], [0]]\n",
    "\n",
    "print(tf.convert_to_tensor(X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Struktur ist, wie bei Pytorch ein neuronales Netz, welches mehrere Layer enthält. Auch hier wird ```Sequential``` zum Stapeln von Layern genutzt.\n",
    "```Flatten``` wird auch genutzt um die Dimension der Eingabe anzupassen.\n",
    "Die voll vernetzten Layer werden hier ```Dense``` genannt. Ein unterschied zu Pytorch ist, dass die Aktivierunsfunktionen hier keine einzelnen Layer sind, sondern bei den Layern direkt mit angegeben werden können.\n",
    "\n",
    "Es wird auch gezeigt, wie man die gewichte eines Layers initialisieren kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, \n",
    "                        activation='relu',\n",
    "                        kernel_initializer=tf.keras.initializers.RandomUniform(-1., 1.),\n",
    "                        bias_initializer=tf.keras.initializers.RandomUniform(-1., 1.)),\n",
    "  tf.keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Abschnitt zeigt, wie man den Optimizer und die Loss Funktion nutzen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf_loss_fn,\n",
    "              metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
